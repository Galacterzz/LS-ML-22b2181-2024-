{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dfcdd46c-adb7-449a-bb9a-595f67aa8bb6",
   "metadata": {},
   "source": [
    "# Required imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2c5ef32d-9612-4bc9-b170-d254d728f5e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np # For performing fast vector calculations\n",
    "import tensorflow as tf\n",
    "from PIL import Image # To load the .bmp image files\n",
    "import os # to manage folder and file paths\n",
    "from sklearn.model_selection import train_test_split # to make training and testing data split\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from sklearn.utils import shuffle"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c0401b2-38a3-4e14-8f59-9e137a97d3af",
   "metadata": {},
   "source": [
    "# Data PreProcessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "05aabdc3-11e1-4799-a6ef-c1923d161b99",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_image(folder_path, label, dimension = (64, 64), flatten = True):\n",
    "    \"\"\" This function will load the images from the file, then normalize,\n",
    "        flatten and adds label to the images. It flattens images if we use\n",
    "        Deense NN, otherwise could keep it to 3d or 2d if we use Conv2D or Conv3D \n",
    "        layers\"\"\"\n",
    "    images = [] # empty list to store the images\n",
    "    labels = [] # empty list to store labels of the images\n",
    "\n",
    "    # lopping through each file in the folder\n",
    "    for filename in os.listdir(folder_path):\n",
    "        if filename.startswith('.'):\n",
    "            # Skip hidden files and directories\n",
    "            continue\n",
    "        # accessing the path of the image file\n",
    "        img_path = os.path.join(folder_path, filename)\n",
    "        # loading image and resizing it to 64x64\n",
    "        img = Image.open(img_path)\n",
    "        img = img.resize(dimension)\n",
    "        # converting image to numpy array\n",
    "        image_array = np.array(img)\n",
    "        # Normalizing the array\n",
    "        image_array = image_array / 255\n",
    "        # flattening the array if specified\n",
    "        if flatten:\n",
    "            image_array = image_array.flatten()\n",
    "        images.append(image_array)\n",
    "        labels.append(label)\n",
    "\n",
    "    return images, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f347267b-f782-48cc-b14b-4f7e22024938",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_path_1 = r'C:\\Users\\ASUS\\OneDrive\\Desktop\\desktop folders\\TECH\\LS\\ML\\code\\week 2\\NN Assign\\homer_bart\\Bart'\n",
    "# label_1 = 0\n",
    "image_path_2 = r'C:\\Users\\ASUS\\OneDrive\\Desktop\\desktop folders\\TECH\\LS\\ML\\code\\week 2\\NN Assign\\homer_bart\\Homer'\n",
    "# label_2 = 1\n",
    "dimension = (64, 64)\n",
    "\n",
    "input_1, label_1 = load_image(image_path_1, 0, dimension) # 160 images\n",
    "input_2, label_2 = load_image(image_path_2, 1, dimension) # 109 images\n",
    "\n",
    "images = np.array(input_1 + input_2) # total 269 images\n",
    "labels = np.array(label_1 + label_2) # Each images has 64*64*3 length\n",
    "\n",
    "# Convert labels to one-hot encoding\n",
    "labels = tf.keras.utils.to_categorical(labels, num_classes=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "83e2b456-d101-446f-8a5b-69c22b7ac7d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(images, labels, test_size = 0.1, random_state = 69, shuffle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "97c07b47-98f9-47bf-96fe-0b1db50a8c86",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(242, 12288)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6add603-942e-4901-beb5-d4c83710eed8",
   "metadata": {},
   "source": [
    "# Data Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "335347ec-b029-44bb-8b98-459e4f4586b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define an ImageDataGenerator for augmentation\n",
    "datagen1 = ImageDataGenerator(\n",
    "    rotation_range=10,  # randomly rotate images in the range (degrees, 0 to 20)\n",
    "    width_shift_range=0.10,  # randomly shift images horizontally (fraction of total width)\n",
    "    height_shift_range=0.10,  # randomly shift images vertically (fraction of total height)\n",
    "    shear_range=0.2,  # shear intensity (shear angle in counter-clockwise direction in degrees)\n",
    "    zoom_range=0.2,  # zoom range [lower, upper] for random zoom\n",
    "    horizontal_flip=True,  # randomly flip images horizontally\n",
    "    vertical_flip=False,  # randomly flip images vertically\n",
    "    fill_mode='nearest'  # fill mode for filling in newly created pixels\n",
    ")\n",
    "\n",
    "datagen2 = ImageDataGenerator(\n",
    "    rotation_range=20,  # randomly rotate images in the range (degrees, 0 to 20)\n",
    "    width_shift_range=0.15,  # randomly shift images horizontally (fraction of total width)\n",
    "    height_shift_range=0.15,  # randomly shift images vertically (fraction of total height)\n",
    "    shear_range=0.2,  # shear intensity (shear angle in counter-clockwise direction in degrees)\n",
    "    zoom_range=0.2,  # zoom range [lower, upper] for random zoom\n",
    "    horizontal_flip=True,  # randomly flip images horizontally\n",
    "    vertical_flip=False,  # randomly flip images vertically\n",
    "    fill_mode='nearest'  # fill mode for filling in newly created pixels\n",
    ")\n",
    "\n",
    "datagen3 = ImageDataGenerator(\n",
    "    rotation_range=30,  # randomly rotate images in the range (degrees, 0 to 20)\n",
    "    width_shift_range=0.20,  # randomly shift images horizontally (fraction of total width)\n",
    "    height_shift_range=0.20,  # randomly shift images vertically (fraction of total height)\n",
    "    shear_range=0.2,  # shear intensity (shear angle in counter-clockwise direction in degrees)\n",
    "    zoom_range=0.2,  # zoom range [lower, upper] for random zoom\n",
    "    horizontal_flip=True,  # randomly flip images horizontally\n",
    "    vertical_flip=False,  # randomly flip images vertically\n",
    "    fill_mode='nearest'  # fill mode for filling in newly created pixels\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9f951dc6-81f2-4cc3-97a3-fb9004a10057",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to augment images\n",
    "def augment_images1(images, labels):\n",
    "    augmented_images = []\n",
    "    augmented_labels = []\n",
    "    \n",
    "    for image, label in zip(images, labels):\n",
    "        # Reshape the image back to 2D (assuming it's flattened)\n",
    "        image = image.reshape(64, 64, 3)\n",
    "        # Expand dimensions to fit the datagen.flow requirement\n",
    "        image = np.expand_dims(image, axis=0)\n",
    "        \n",
    "        # Generate augmented images\n",
    "        aug_iter = datagen1.flow(image, batch_size=1)\n",
    "        augmented_image = next(aug_iter)[0].astype(np.float32)\n",
    "        \n",
    "        # Flatten the augmented image back\n",
    "        augmented_image = augmented_image.flatten()\n",
    "        \n",
    "        augmented_images.append(augmented_image)\n",
    "        augmented_labels.append(label)  # Use the original label for augmented image\n",
    "    \n",
    "    return np.array(augmented_images), np.array(augmented_labels)\n",
    "\n",
    "# Function to augment images\n",
    "def augment_images2(images, labels):\n",
    "    augmented_images = []\n",
    "    augmented_labels = []\n",
    "    \n",
    "    for image, label in zip(images, labels):\n",
    "        # Reshape the image back to 2D (assuming it's flattened)\n",
    "        image = image.reshape(64, 64, 3)\n",
    "        # Expand dimensions to fit the datagen.flow requirement\n",
    "        image = np.expand_dims(image, axis=0)\n",
    "        \n",
    "        # Generate augmented images\n",
    "        aug_iter = datagen2.flow(image, batch_size=1)\n",
    "        augmented_image = next(aug_iter)[0].astype(np.float32)\n",
    "        \n",
    "        # Flatten the augmented image back\n",
    "        augmented_image = augmented_image.flatten()\n",
    "        \n",
    "        augmented_images.append(augmented_image)\n",
    "        augmented_labels.append(label)  # Use the original label for augmented image\n",
    "    \n",
    "    return np.array(augmented_images), np.array(augmented_labels)\n",
    "\n",
    "# Function to augment images\n",
    "def augment_images3(images, labels):\n",
    "    augmented_images = []\n",
    "    augmented_labels = []\n",
    "    \n",
    "    for image, label in zip(images, labels):\n",
    "        # Reshape the image back to 2D (assuming it's flattened)\n",
    "        image = image.reshape(64, 64, 3)\n",
    "        # Expand dimensions to fit the datagen.flow requirement\n",
    "        image = np.expand_dims(image, axis=0)\n",
    "        \n",
    "        # Generate augmented images\n",
    "        aug_iter = datagen3.flow(image, batch_size=1)\n",
    "        augmented_image = next(aug_iter)[0].astype(np.float32)\n",
    "        \n",
    "        # Flatten the augmented image back\n",
    "        augmented_image = augmented_image.flatten()\n",
    "        \n",
    "        augmented_images.append(augmented_image)\n",
    "        augmented_labels.append(label)  # Use the original label for augmented image\n",
    "    \n",
    "    return np.array(augmented_images), np.array(augmented_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "df881f80-0fa2-4804-be69-4c0f357a11e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Apply augmentation to training set\n",
    "# X_train_augmented1, y_train_augmented1 = augment_images1(X_train, y_train)\n",
    "# X_train_augmented2, y_train_augmented2 = augment_images2(X_train, y_train)\n",
    "# X_train_augmented3, y_train_augmented3 = augment_images3(X_train, y_train)\n",
    "\n",
    "# # Concatenate original and augmented data\n",
    "# X_train = np.concatenate((X_train, X_train_augmented1), axis=0)\n",
    "# y_train = np.concatenate((y_train, y_train_augmented1), axis=0)\n",
    "\n",
    "# X_train = np.concatenate((X_train, X_train_augmented2), axis=0)\n",
    "# y_train = np.concatenate((y_train, y_train_augmented2), axis=0)\n",
    "\n",
    "# X_train = np.concatenate((X_train, X_train_augmented3), axis=0)\n",
    "# y_train = np.concatenate((y_train, y_train_augmented3), axis=0)\n",
    "\n",
    "# Shuffle the augmented data\n",
    "X_train, y_train = shuffle(X_train, y_train, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "45ee8e8a-d71a-4b3c-9d0e-f704880b6dac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(242, 12288)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d06ccc55-a4a2-4cb3-97ed-26e6a20cff19",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\python311\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:88: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                         </span>┃<span style=\"font-weight: bold\"> Output Shape                </span>┃<span style=\"font-weight: bold\">         Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
       "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)                 │       <span style=\"color: #00af00; text-decoration-color: #00af00\">6,291,968</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)                 │          <span style=\"color: #00af00; text-decoration-color: #00af00\">65,664</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dense_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)                 │          <span style=\"color: #00af00; text-decoration-color: #00af00\">16,512</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dense_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>)                   │             <span style=\"color: #00af00; text-decoration-color: #00af00\">258</span> │\n",
       "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                        \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape               \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
       "│ dense (\u001b[38;5;33mDense\u001b[0m)                        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)                 │       \u001b[38;5;34m6,291,968\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)                 │          \u001b[38;5;34m65,664\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dense_2 (\u001b[38;5;33mDense\u001b[0m)                      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)                 │          \u001b[38;5;34m16,512\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dense_3 (\u001b[38;5;33mDense\u001b[0m)                      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2\u001b[0m)                   │             \u001b[38;5;34m258\u001b[0m │\n",
       "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">6,374,402</span> (24.32 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m6,374,402\u001b[0m (24.32 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">6,374,402</span> (24.32 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m6,374,402\u001b[0m (24.32 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Dense(512, input_shape = X_train[0].shape, activation = 'relu', kernel_regularizer = tf.keras.regularizers.l2(0.001)),\n",
    "    # tf.keras.layers.Dense(512, input_shape = X_train[0].shape, activation = 'relu'),\n",
    "    # tf.keras.layers.Dropout(0.5),\n",
    "    tf.keras.layers.Dense(128, activation = 'relu', kernel_regularizer = tf.keras.regularizers.l2(0.001)),\n",
    "    # tf.keras.layers.Dense(256, activation = 'relu'),\n",
    "    # tf.keras.layers.Dropout(0.5),\n",
    "    tf.keras.layers.Dense(128, activation = 'relu', kernel_regularizer = tf.keras.regularizers.l2(0.001)),\n",
    "    # tf.keras.layers.Dense(128, activation = 'relu'),\n",
    "    # tf.keras.layers.Dense(20, activation = 'relu', kernel_regularizer = tf.keras.regularizers.l2(0.001)),\n",
    "    tf.keras.layers.Dense(2, activation = 'softmax')\n",
    "])\n",
    "\n",
    "model.compile(\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate = 0.0001),\n",
    "    # optimizer = 'adam',\n",
    "    # loss = 'binary_crossentropy',\n",
    "    loss = 'categorical_crossentropy',\n",
    "    metrics = ['accuracy']\n",
    ")\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "df9449fc-77c8-4d31-943e-3bc8ba42cedf",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/35\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 94ms/step - accuracy: 0.5152 - loss: 2.4623\n",
      "Epoch 2/35\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - accuracy: 0.4553 - loss: 2.1822\n",
      "Epoch 3/35\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 101ms/step - accuracy: 0.5701 - loss: 1.9341\n",
      "Epoch 4/35\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 100ms/step - accuracy: 0.7265 - loss: 1.8244\n",
      "Epoch 5/35\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 87ms/step - accuracy: 0.7877 - loss: 1.7774\n",
      "Epoch 6/35\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 95ms/step - accuracy: 0.7447 - loss: 1.7554\n",
      "Epoch 7/35\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 89ms/step - accuracy: 0.7196 - loss: 1.7751\n",
      "Epoch 8/35\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 85ms/step - accuracy: 0.8035 - loss: 1.6457\n",
      "Epoch 9/35\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 85ms/step - accuracy: 0.8003 - loss: 1.6283\n",
      "Epoch 10/35\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 85ms/step - accuracy: 0.7883 - loss: 1.6427\n",
      "Epoch 11/35\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 87ms/step - accuracy: 0.8178 - loss: 1.5553\n",
      "Epoch 12/35\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 85ms/step - accuracy: 0.8353 - loss: 1.5639\n",
      "Epoch 13/35\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 96ms/step - accuracy: 0.7793 - loss: 1.5929\n",
      "Epoch 14/35\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 100ms/step - accuracy: 0.7915 - loss: 1.5631\n",
      "Epoch 15/35\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - accuracy: 0.8110 - loss: 1.5231\n",
      "Epoch 16/35\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 103ms/step - accuracy: 0.8323 - loss: 1.4982\n",
      "Epoch 17/35\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 104ms/step - accuracy: 0.8798 - loss: 1.4214\n",
      "Epoch 18/35\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - accuracy: 0.8429 - loss: 1.4309\n",
      "Epoch 19/35\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 87ms/step - accuracy: 0.8778 - loss: 1.3885\n",
      "Epoch 20/35\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 88ms/step - accuracy: 0.8754 - loss: 1.3733\n",
      "Epoch 21/35\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 88ms/step - accuracy: 0.8372 - loss: 1.3483\n",
      "Epoch 22/35\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 87ms/step - accuracy: 0.8759 - loss: 1.3309\n",
      "Epoch 23/35\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 87ms/step - accuracy: 0.8682 - loss: 1.3800\n",
      "Epoch 24/35\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 85ms/step - accuracy: 0.8047 - loss: 1.4063\n",
      "Epoch 25/35\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 85ms/step - accuracy: 0.8411 - loss: 1.3564\n",
      "Epoch 26/35\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 85ms/step - accuracy: 0.9051 - loss: 1.2796\n",
      "Epoch 27/35\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 87ms/step - accuracy: 0.8805 - loss: 1.2835\n",
      "Epoch 28/35\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 88ms/step - accuracy: 0.9122 - loss: 1.2364\n",
      "Epoch 29/35\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - accuracy: 0.9264 - loss: 1.2109\n",
      "Epoch 30/35\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 86ms/step - accuracy: 0.9412 - loss: 1.1869\n",
      "Epoch 31/35\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 88ms/step - accuracy: 0.9200 - loss: 1.1913\n",
      "Epoch 32/35\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 95ms/step - accuracy: 0.9195 - loss: 1.1725\n",
      "Epoch 33/35\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 105ms/step - accuracy: 0.9127 - loss: 1.1764\n",
      "Epoch 34/35\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 101ms/step - accuracy: 0.9428 - loss: 1.1249\n",
      "Epoch 35/35\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 88ms/step - accuracy: 0.9670 - loss: 1.1159\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x263b4602110>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train, y_train, batch_size = 32, epochs = 35, verbose = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "37227a59-65f3-42ef-a577-116392067f6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 - 0s - 136ms/step - accuracy: 0.8148 - loss: 1.4158\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_accuracy = model.evaluate(X_test, y_test, verbose = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e5082ae-c671-49a2-b309-bdb75a3b75a2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "515fc221-22a9-455c-b00b-2a0452451781",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
